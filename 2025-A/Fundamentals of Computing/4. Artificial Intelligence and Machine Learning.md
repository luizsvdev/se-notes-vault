# 4.1 Perceptron Formula  
The perceptron is a simple type of artificial neuron used for binary classification. Its output is determined by the following equation:

$$
y = f\left( \sum (x_i \cdot w_i) + \text{bias} \right)
$$

Where:  
- $x_i$ is the input feature  
- $w_i$ is the weight associated with the input feature  
- $\text{bias}$ is a constant that shifts the activation function, allowing the model to make predictions even when all inputs are zero  

The **bias** helps the model learn patterns that are not centered at the origin, providing greater flexibility for the decision boundary.

# 4.2 Step Activation Function  
The activation function determines the final **output** based on the weighted sum:

$$
O =
\begin{cases} 
+1, & d \geq 0 \\
-1, & d < 0
\end{cases}
$$

Where $d$ is the weighted sum of inputs plus the bias.

# 4.3 Weight Update Rule  
The perceptron learning rule updates weights to minimize classification errors:

$$
\Delta w_i = k \cdot e \cdot x_i
$$
$$
w_i^{\text{new}} = w_i + \Delta w_i
$$

Where:  
- $k$ is the learning rate  
- $e$ is the error, calculated as:

$$
e = \text{target} - O
$$

- $x_i$ is the input feature  

The weights are adjusted iteratively until the perceptron correctly classifies all training samples or reaches a stopping criterion.
