## Perceptron Formula 
$$y = f\left( \sum (x_i \cdot w_i) + \ bias\right)$$
Where:
* $x_i$ is the input feature (entry)
* $w_i$ is the weight associated with the input feature
* $bias$ is a constant that helps shift the activation function, enabling the model to make predictions even when all inputs are zero

The **bias** helps the model learn patterns that aren't centered at the origin, offering greater flexibility for the decision boundary.

### Step Activation Function

With the result of the sum, we can use a **step activation function**, which is responsible for determining the ***output***:

$O = +1$ if $d \geq 0$
$O = -1$ if $d < 0$

### Example
$$\Delta w_i = k \cdot e \cdot x_i$$
$$w_{i}^{new} = w_i + \Delta{w_i}$$